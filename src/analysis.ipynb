{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "# hyperparameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "l2_regularization_lambda = 0.1\n",
    "\n",
    "# preprocessing\n",
    "data = pd.read_csv(\"../data/crowdbabble_data_processed.csv\")\n",
    "data.columns = [str.lower(i.replace(' ', '_').replace('?', '')) for i in list(data.columns)]\n",
    "data.start_date = [datetime.strptime(i, '%m/%d/%y').strftime(\"%j\") for i in data.start_date]\n",
    "data.end_date = [datetime.strptime(i, '%m/%d/%y').strftime(\"%j\") for i in data.end_date]\n",
    "\n",
    "explanatory_data = data[['channel_id', 'customer_segment_id', 'location',\n",
    "                       'campaign_id', 'marketing_goal_id', \n",
    "                       'start_date', 'end_date', 'video_campaign']]\n",
    "\n",
    "response_data = data[['cpm', 'cpc', 'cpa', 'cpv']].as_matrix()\n",
    "encoded_explanatory_data = pd.get_dummies(explanatory_data, \n",
    "                                          columns=['channel_id', 'customer_segment_id', 'location',\n",
    "                                                   'campaign_id', 'marketing_goal_id', \n",
    "                                                   'start_date', 'end_date'])\n",
    "encoded_explanatory_data_matrix = encoded_explanatory_data.as_matrix()\n",
    "\n",
    "# x_data, x_test, y_data, y_test = train_test_split(encoded_explanatory_data, response_data, test_size = .3)\n",
    "# x_train, x_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size = 0.5)\n",
    "# np.save(\"../bin/x_valid\", x_valid)\n",
    "# np.save(\"../bin/y_valid\", y_valid)\n",
    "# np.save(\"../bin/x_train\", x_train)\n",
    "# np.save(\"../bin/y_train\", y_train)\n",
    "# np.save(\"../bin/x_test\", x_test)\n",
    "# np.save(\"../bin/y_test\", y_test)\n",
    "\n",
    "x_train = np.load(\"../bin/x_train.npy\")\n",
    "y_train = np.load(\"../bin/y_train.npy\")\n",
    "x_test = np.load(\"../bin/x_test.npy\")\n",
    "y_test = np.load(\"../bin/y_test.npy\")\n",
    "x_valid = np.load(\"../bin/x_valid.npy\")\n",
    "y_valid = np.load(\"../bin/y_valid.npy\")\n",
    "\n",
    "# set up tensorflow graph\n",
    "inputs = tf.placeholder(tf.float32, [None, x_train.shape[1]])\n",
    "answers = tf.placeholder(tf.float32, [None, y_train.shape[1]])\n",
    "\n",
    "l1 = tf.contrib.layers.fully_connected(inputs, \n",
    "                                      50,\n",
    "                                      activation_fn=None)\n",
    "l1 = tf.contrib.layers.fully_connected(l1,\n",
    "                                       10,\n",
    "                                       activation_fn=None)\n",
    "output = tf.contrib.layers.fully_connected(l1, \n",
    "                                      y_train.shape[1],\n",
    "                                      activation_fn=tf.nn.relu)\n",
    "loss = tf.reduce_mean(tf.squared_difference(output, answers))\n",
    "# L2 regularization\n",
    "for v in tf.trainable_variables():\n",
    "    if not 'bias' in v.name.lower():\n",
    "        loss += l2_regularization_lambda * tf.nn.l2_loss(v)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../bin/saved-models/approach_1_best_model.ckpt\n",
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"../bin/saved-models/approach_1_best_model.ckpt.meta\"):\n",
    "    saver.restore(sess, \"../bin/saved-models/approach_1_best_model.ckpt\")\n",
    "    print(\"Model restored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 0.00% complete, 0 mins, Avg loss: 793.10413\n",
      "Epoch 50: 5.00% complete, 0 mins, Avg loss: 564.70636\n",
      "Epoch 100: 10.00% complete, 0 mins, Avg loss: 295.18088\n",
      "Epoch 150: 15.00% complete, 0 mins, Avg loss: 234.74316\n",
      "Epoch 200: 20.00% complete, 1 mins, Avg loss: 211.95674\n",
      "Epoch 250: 25.00% complete, 1 mins, Avg loss: 193.82092\n",
      "Epoch 300: 30.00% complete, 1 mins, Avg loss: 180.55537\n",
      "Epoch 350: 35.00% complete, 2 mins, Avg loss: 172.71385\n",
      "Epoch 400: 40.00% complete, 2 mins, Avg loss: 169.14183\n",
      "Epoch 450: 45.00% complete, 2 mins, Avg loss: 167.68564\n",
      "Epoch 500: 50.00% complete, 3 mins, Avg loss: 166.88458\n",
      "Epoch 550: 55.00% complete, 3 mins, Avg loss: 166.17982\n",
      "Epoch 600: 60.00% complete, 3 mins, Avg loss: 165.39836\n",
      "Epoch 650: 65.00% complete, 4 mins, Avg loss: 164.48012\n",
      "Epoch 700: 70.00% complete, 4 mins, Avg loss: 163.39299\n",
      "Epoch 750: 75.00% complete, 4 mins, Avg loss: 162.13490\n",
      "Epoch 800: 80.00% complete, 5 mins, Avg loss: 160.75475\n",
      "Epoch 850: 85.00% complete, 5 mins, Avg loss: 159.35727\n",
      "Epoch 900: 90.00% complete, 5 mins, Avg loss: 158.07303\n",
      "Epoch 950: 95.00% complete, 5 mins, Avg loss: 157.00735\n",
      "Duration: 6 mins\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "start_time = time.time()\n",
    "best_valid_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss, _ = sess.run((loss, optimizer), feed_dict = {inputs:x_train, answers:y_train})\n",
    "    if epoch % 5 == 0:\n",
    "        valid_loss = sess.run(loss, feed_dict = {inputs:x_valid, answers:y_valid})\n",
    "        if valid_loss < (best_valid_loss - 0.5):\n",
    "            best_valid_loss = valid_loss\n",
    "            save_path = saver.save(sess, \"saved-models/approach_1_best_model.ckpt\")\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch %d: %.2f%% complete, %d mins, Avg loss: %.5f\" % (epoch, \n",
    "                                                                    epoch*100.0/num_epochs,\n",
    "                                                                    (time.time() - start_time)/60,\n",
    "                                                                    epoch_loss))                      \n",
    "print(\"Duration: %d mins\" % int((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 157.14\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "test_loss, prediction = sess.run((loss, output), feed_dict = {inputs:x_test, answers:y_test})\n",
    "print(\"MSE: %.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marketing_goal_names = ['Brand Engagement', 'Lead Generation', 'Acquisition', 'Discount coupons', 'Local/other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# come up with all possible combinations of 'channel_id', 'customer_segment_id', 'location'\n",
    "def create_combinations(campaign_id, marketing_goal_id, video_campaign):\n",
    "    combinations = list(itertools.product(range(10), range(20), range(199)))\n",
    "    dataset = np.zeros((len(combinations), x_train.shape[1]))\n",
    "    for idx, combination in enumerate(combinations):\n",
    "        dataset[idx, 0] = video_campaign\n",
    "        dataset[idx, 1 + combination[0]] = 1\n",
    "        dataset[idx, 1 + 10 + combination[1]] = 1\n",
    "        dataset[idx, 1 + 10 + 20 + combination[2]] = 1\n",
    "        dataset[idx, 1 + 10 + 20 + 199 + campaign_id - 1] = 1\n",
    "        dataset[idx, 1 + 10 + 20 + 199 + 4 + marketing_goal_id - 1] = 1\n",
    "        # assuming dates don't have an effect on costs. just choose first date for start and first for end\n",
    "        start_date_id = end_date_id = 0\n",
    "        dataset[idx, 1 + 10 + 20 + 199 + 4 + 5 + start_date_id] = 1\n",
    "        dataset[idx, 1 + 10 + 20 + 199 + 4 + 5 + 76 + end_date_id] = 1\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Come up with best combination of channel ID, customer segment, and location for a particular \n",
    "# combination of campaign ID, marketing goal, and video/non video campaign\n",
    "def get_optimal_strategy(campaign_id, marketing_goal_id, video_campaign):\n",
    "    combinations_dataset = create_combinations(campaign_id, marketing_goal_id, video_campaign)\n",
    "    fake_answers = np.zeros((combinations_dataset.shape[0], y_test.shape[1]))\n",
    "    prediction = sess.run(output, feed_dict = {inputs:combinations_dataset, answers:fake_answers})\n",
    "    \n",
    "    # See which combination gives you the lowest CPM\n",
    "    best_combination_idx = np.argmin(prediction[:,0])\n",
    "    predicted_cpm_of_best_combination = prediction[best_combination_idx][0]\n",
    "    \n",
    "    ## find the observation from the real dataset that has the exact same combination\n",
    "    encoded_explanatory_data_matrix_without_dates = encoded_explanatory_data_matrix[:, 0:(1 + 10 + 20 + 199 + 4 + 5)]\n",
    "    combinations_dataset_without_dates = combinations_dataset[:, 0:(1 + 10 + 20 + 199 + 4 + 5)]\n",
    "    best_combination_from_combinations_dataset_without_dates = combinations_dataset_without_dates[best_combination_idx, :]\n",
    "    \n",
    "    real_observation_of_best_combination_idx = np.where((encoded_explanatory_data_matrix_without_dates == best_combination_from_combinations_dataset_without_dates).all(axis=1))[0]\n",
    "    # if there is no actual observation to compare the best combination to, just use the predicted one\n",
    "    if real_observation_of_best_combination_idx.size == 0:\n",
    "        real_cpm_of_best_combination = predicted_cpm_of_best_combination\n",
    "    # otherwise use the actual observation\n",
    "    else:\n",
    "        real_observation_of_best_combination = encoded_explanatory_data_matrix_without_dates[real_observation_of_best_combination_idx, :]\n",
    "        # get its CPM\n",
    "        real_cpm_of_best_combination = response_data[real_observation_of_best_combination_idx, 0]\n",
    "    \n",
    "    \n",
    "    ## find other combinations in the real dataset to compare the best combination with\n",
    "    column_ids_of_constants = [0] + range(1 + 10 + 20 + 199, 1 + 10 + 20 + 199 + 4, 1) + range(1 + 10 + 20 + 199 + 4, 1 + 10 + 20 + 199 + 4 + 5, 1)\n",
    "    encoded_explanatory_data_matrix_only_constants = encoded_explanatory_data_matrix[:, column_ids_of_constants]\n",
    "    best_combination_constants = best_combination_from_combinations_dataset_without_dates[column_ids_of_constants]\n",
    "    # find observations from the real dataset that have combinations that are slightly different from that of the recommended combination\n",
    "    real_observations_of_suboptimal_combinations_idx = np.where((encoded_explanatory_data_matrix_only_constants == best_combination_constants).all(axis=1))[0]\n",
    "    # get their cpms\n",
    "    cpms_of_suboptimal_combinations = response_data[real_observations_of_suboptimal_combinations_idx,0]\n",
    "\n",
    "    # find out what % of the other combinations the recommended combination performs better than\n",
    "    how_many_combinations_is_best_combination_better_than = np.mean(real_cpm_of_best_combination > cpms_of_suboptimal_combinations)\n",
    "    # find how_much_better the_best_combination is than_other_combinations\n",
    "    how_much_better_is_the_best_combination_than_other_combinations_in_percent = 100.0*(real_cpm_of_best_combination/np.mean(cpms_of_suboptimal_combinations) - 1)\n",
    "    # get spread of cpm of suboptimal combinations\n",
    "    cpm_spread_of_suboptimal_combinations = np.percentile(cpms_of_suboptimal_combinations, [100, 75, 50, 25, 0])\n",
    "\n",
    "    # convert combination into human readable format\n",
    "    optimal_channel_id = np.where(best_combination_from_combinations_dataset_without_dates[1:11] == 1)[0][0] + 1\n",
    "    optimal_customer_segment_id = np.where(best_combination_from_combinations_dataset_without_dates[11:31] == 1)[0][0] + 1\n",
    "    optimal_location = np.where(best_combination_from_combinations_dataset_without_dates[31:230] == 1)[0][0] + 1\n",
    "\n",
    "    print(\"For a %s video campaign with a Campaign ID of %d, and a Marketing Goal of %s, the optimal combination is:\\n\\nChannel ID: %d\\nCustomer Segment ID: %d\\nLocation: %d\\n\\nThis combination would give a CPM of $%.2f, performing %.2f%% better than other approaches.\" %\n",
    "         (('' if video_campaign else 'non'), \n",
    "          campaign_id, \n",
    "          marketing_goal_names[marketing_goal_id - 1], \n",
    "          optimal_channel_id, \n",
    "          optimal_customer_segment_id, \n",
    "          10000+optimal_location, \n",
    "          real_cpm_of_best_combination,\n",
    "          how_much_better_is_the_best_combination_than_other_combinations_in_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a non video campaign with a Campaign ID of 3, and a Marketing Goal of Local/other, the optimal combination is:\n",
      "\n",
      "Channel ID: 8\n",
      "Customer Segment ID: 11\n",
      "Location: 10182\n",
      "\n",
      "This combination would give a CPM of $5.63, performing 2.08% better than other approaches.\n"
     ]
    }
   ],
   "source": [
    "# Arguments: campaign_id (1 - 4), marketing_goal_id (1 - 5), video_campaign (0 or 1)\n",
    "get_optimal_strategy(3, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552.95256110241712"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE: model doesn't perform much better than simply predicting mean of training responses for all rows\n",
    "mean_prediction = np.ones((y_test.shape)) * np.mean(y_test)\n",
    "np.mean((mean_prediction - y_test)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
